#!/bin/bash
#======================================================
#
# Ricostruzione ambiente per la creazione del WARC delle tesi di dottorato
# Autore:   Argentino Trombin
# Data:     12/07/2019
# NOATA:    Ricostruito da vari pezzi non documentati di Raffaele Messuti trovati
#           sul server md-front02.bncf.lan (192.168.7.151)
#           cartelle:   /home/rmessuti
#                       /mnt/volume2/in-progress
#           https://github.com/depositolegale/ojs-archive
#======================================================
# Prendi l'ambiente dell'utente
#. ~.bash_profile

source scripts/run_main.sh



#======================================
# Start execution
# ./run.sh
# Any missing required subdirectory will be automatically generated
#======================================
cd $HARVEST_DIR
ora="date +%Y-%m-%d-%H:%M:%S"
tmr=$(timer)

echo "===================================================="
echo " Procedura per l'acquisizione delle tesi di "
echo " dottorato o riviste (eJournals)"
# echo " Versione 2019_11_08.1" # aggiunto check_for_bad_seeds_lookup
# echo " Versione 2019_11_18.2" # nuova gestione per gestire spazio disco
# echo " Versione 2019_11_27.1" # eliminazione di seed duplicati (vedi seeds_dup.csv)
# echo " Versione 2019_11_28.1" # gestione ricevute in un unico foglio excel (ok, [ko, no didl resource, missing, url doppie])
                              # nuova gestione report in formato excel
# echo " Versione 2019_12_03.1" # gestione errori in warc filtrati per seed da scaricare
# echo " Versione 2019_12_08.1" # get_warcked_seeds_and_not_from_logs (unificato  le get delle url salvate ed andate in errore)

# echo " Versione 2019_12_09.1"   # Rifatto index_warcs per gestire + volumi usando i symbolic links
                                # Pulizia indici tramite liste seed da scaricare
# echo " Versione 2020_23_01.1" # Generazione lista delle tesi in ricevuta
echo " Versione 2021_01_25.1"	# Prove con wget recursive
# echo " Versione 2019_12_08.1"
# echo " Versione 2019_12_08.1"


echo " `${ora}` Inizio processo di run.sh"
echo "==================================================="

# -------------
# PRELIMINARIES
# -------------
check_arguments "$@"
init_variables
print_configuration
# ----
# MAIN
# ----

# harvest_metadata

# createSeeds

# TODO try wget with option --warc-max-size=NUMBER   to limit file size. Eg. --warc-max-size=2G

# create_warcs_concurrently

# create_warcs_md5

# Solo per TESI. Controllo non possibile per gli eJournal
# check_pdf_download

# Makes destination warcs read only (to avoid deletion)
# copy_warcs_and_logs_to_destination_dir_and_remove

# index_warcs

# get_indexes_for_compression

# Rinominare a mano con .embargo. i siti sotto embargo

# compress_parziale=0
# compress_completo=1
#
# compress_warc_indexes $compress_completo
# replace_warc_indexes_with_compressed_ones_in_index
#
# compress_warc_indexes $compress_parziale
# replace_warc_indexes_with_compressed_ones_in_memoria


# # Per fare test della wayback machine in ambiente di collaudo/esercizio
# # sudo service supervisor stop
# # sudo service supervisor start
# # sudo service supervisor restart

# # Per fare test della wayback machine in ambiente locale
# cd /home/argentino/magazzini_digitali/wayback
# wayback   (will run on port 8080)


# get_warcked_seeds_and_not_from_logs
# check_for_missing_seeds
# check_for_harvest_mismatch

# generate_nbn_identifiers


# ==========================================
# S3 warc.gz upload / download
# Prima di fare le ricevute facciamo il salvataggio dei dati su S3
#
#
# --------------------------------
# Caricamebnto warcs da harvest Almaviva 
# 
# Si usa il file etd.csv per guidare il caricamento
# multipart_mode=true
# upload_warcs_to_s3 $multipart_mode

# upload harvest CDXJ. Preparare .zip ed md5 a mano
# multipart_mode=true
# file_to_upload="/mnt/volume2/wayback/tools/webarchive-indexing/tmp/cdxj.zip"
# md5_file_to_upload=$file_to_upload".md5"
# s3_path_filename="harvest/2020_08_05_tesi/indexes/cdxj.zip"
# log_filename="cdxj.upload.log"
# upload_file_to_s3 $multipart_mode $file_to_upload $md5_file_to_upload $s3_path_filename $log_filename




#
# Solo per file molto grandii > 15GB (split e nd5 manuale per il momento)
# upload_split_warcs_to_s3
#
# -------------------------------------
# !!!! Caricamento warcs da harvest precedenti fino al 10/2018
# 
# ONCE ONLY!!!
# -- prepare_etd_warcs_list_to_upload()
# from_line=1
# to_line=1
# multipart_mode=false
# upload_etd_warcs_to_S3	$from_line $to_line $multipart_mode
#
# --------------------------
# download_warcs_from_s3

# --------------------------
# prepare_harvest_record_AV
# scripts/DbUpdateInsertS3.sh
# prepare_harvest_record_cdxj
# scripts/DbUpdateInsertS3.sh

# prepare_harvest_record_storico	1	5
# prepare_harvest_record_cdx_storico
# scripts/DbUpdateInsertS3.sh

# END S3 warc.gz upload / download
# ---------------------------------

# 09/10/2020
# NMB per il frontend devono stare sul frontend nella cartella: /var/www/index.depositolegale.it/ricevute
# 
# make_receipts

# check_for_receipts_mismatch ### DA RIVEDERE forse sostituibile con check_match_seeds_donloaded_to_download ()


# Troviamo documenti sotto embargo (SOLO per Tesi)
# find_embargoed

# Rimuoviamo le tesi alle quali hanno tolto l'embargo
# scripts/DbDeleteUnembargoed.sh

# Carichiamo le tesi sotto embargo sul db
# scripts/DbUpdateInsert.sh


# UNIMARC
# --------

# Ricordarsi di resettare il contatore quando si fa l'unimarc finale!! tesi/09_unimarcs/ctr_001.txt
# Se file non esiste viene creato con ctr a 1
# 13/02/2020 Rossana - unimarc genera comunque link a memoria

# GESTIRE RECORD CANCELLATI
# createUnimarc

# check_unimarc_for_no_wayback_link

# Rivedere prendendo date da log scarico metadati!!
# generate_last_harvest_list

# make_report

# ===================================
# 27/11/2020 - Creazione file con range di date per l'harvest corrente
# scripts/DbDownload.sh 	# Da schema harvest prendi l'nanagrafe e date (messi in csv)

# generate_harvest_dates_from_metadata_logs

# scripts/DbUpdateInsertHarvestDate.sh 	# Carichiamo range di date per l'ultimo harvest

# generate_last_harvest_dates
# ===================================



# ===================================
# 07/12/2020
# Archiviazione documenti in Magazzini Digitali
# ===================================
# prepare_docs_for_MD


# ---------------
# TESTING
# ---------------



# ---------------
# END TESTING
# ---------------





echo
echo "Elapsed time $(timer $tmr)"
echo `${ora}` "Fine processo di run.sh"
echo
echo =================
echo FINE ELABORAZIONE
echo =================

# Start Stop wayback in esercizio
#   sudo service supervisor stop
#   sudo service supervisor start

# Per cercare in memporia senza dover digitare ma  copincollando
# http://memoria.depositolegale.it/*/URL
# http://memoria.depositolegale.it/*/http://amsdottorato.unibo.it/480/    fino 2018
# http://memoria.depositolegale.it/*/http://amsdottorato.unibo.it/2206/   2019 (trovato 2015) nell'harvest del 05/11/19 questa url non e' stata scaricata


# =====================
# OBSOLETE 
# prepare_wget_sites_list

# =====================
# PROCEDURE UNA TANTUM 
# 
# Solo se non abbiamo generato gli md5 precedentemente!!!
# create_dest_warcs_md5

# ---------------
#  archived thesis per site
# ---------------
# find_archived_thesis
# Procedura da farsi una tantum per generare numero NBN delle tesi archiviate in memoria (storico)
# generate_archived_thesis_nbn

# Procedura da farsi una tantum per recuperare il pregresso (storico) da DB MySql di BNCF
# create_unimarc_from_dublin_core

# 22/12/2020
# Procedura da farsi una tantum per splittare warc.gz troppo grandi da caricare unzippati su S3
# Warc coinvolti solo alcuni dell'harvest del 05/08/2020
# split_warcs

# END PROCEDURE UNA TANTUM 
# =====================

